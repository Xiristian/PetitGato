# Databricks notebook source
# MAGIC %md
# MAGIC ##Validando a SparkSession

# COMMAND ----------

spark

# COMMAND ----------

# MAGIC %md
# MAGIC ##Conectando Azure ADLS Gen2 no Databricks

# COMMAND ----------

# MAGIC %md
# MAGIC ###Mostrando os pontos de montagem no cluster Databricks

# COMMAND ----------

display(dbutils.fs.mounts())

# COMMAND ----------

# MAGIC %md
# MAGIC ### Definindo uma função para montar um ADLS com um ponto de montagem com ADLS SAS 

# COMMAND ----------

storageAccountName = "datalake271297fb88ef6733"
sasToken = "sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2024-06-29T00:29:26Z&st=2024-06-28T16:29:26Z&spr=https&sig=NPxuYKGVwZPiye%2FpKprMavdDoJLyIFt5WfkJE40TNFE%3D"

# COMMAND ----------

# MAGIC %md
# MAGIC ### Mostrando todos os arquivos da camada prata

# COMMAND ----------

display(dbutils.fs.ls(f"/mnt/{storageAccountName}/silver"))

# COMMAND ----------

# MAGIC %md
# MAGIC ###Gerando um dataframe dos delta lake no container prata do Azure Data Lake Storage

# COMMAND ----------

df_customerorder = spark.read.format('delta').load(f"/mnt/{storageAccountName}/silver/customerorder")
df_orderitem     = spark.read.format('delta').load(f"/mnt/{storageAccountName}/silver/orderitem")
df_item          = spark.read.format('delta').load(f"/mnt/{storageAccountName}/silver/item")
df_customer      = spark.read.format('delta').load(f"/mnt/{storageAccountName}/silver/customer")
df_category      = spark.read.format('delta').load(f"/mnt/{storageAccountName}/silver/category")
df_coffeetable   = spark.read.format('delta').load(f"/mnt/{storageAccountName}/silver/coffeetable")
df_payment       = spark.read.format('delta').load(f"/mnt/{storageAccountName}/silver/payment")
df_cat           = spark.read.format('delta').load(f"/mnt/{storageAccountName}/silver/cat")
df_customercat   = spark.read.format('delta').load(f"/mnt/{storageAccountName}/silver/customercat")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Adicionando metadados de data e hora de processamento e nome do arquivo de origem

# COMMAND ----------

# MAGIC %sql
# MAGIC drop table if exists dim_item

# COMMAND ----------

# MAGIC %sql
# MAGIC drop table if exists dim_gato

# COMMAND ----------

# MAGIC %sql
# MAGIC drop table if exists dim_tempo

# COMMAND ----------

# MAGIC %sql
# MAGIC create table dim_item (
# MAGIC   SK_ITEM             bigint generated by default as identity,
# MAGIC   CODIGO_ITEM         integer,
# MAGIC   DESCRICAO           varchar(100),
# MAGIC   CATEGORIA           varchar(100),
# MAGIC   VALOR               numeric(15,2),
# MAGIC   CUSTO               numeric(15,2)
# MAGIC )
# MAGIC USING delta
# MAGIC LOCATION '/mnt/datalakepetitgato/gold/dim_item'

# COMMAND ----------

# MAGIC %sql
# MAGIC create table dim_gato (
# MAGIC   SK_GATO             bigint generated by default as identity,
# MAGIC   CODIGO_GATO         integer,
# MAGIC   CODIGO_CLIENTE         integer,
# MAGIC )
# MAGIC USING delta
# MAGIC LOCATION '/mnt/datalakepetitgato/gold/dim_gato'

# COMMAND ----------

from pyspark.sql.functions import expr, date_format

# Define o intervalo de datas desejado
data_inicial = "2023-01-01"
data_final = "2026-12-31"

# Calcula o número de dias no intervalo
num_dias = spark.sql(f"SELECT datediff('{data_final}', '{data_inicial}')").collect()[0][0]

# Cria um DataFrame com uma coluna contendo uma sequência de datas
df_calendario = spark.range(0, num_dias + 1) \
    .selectExpr(f"date_add(to_date('{data_inicial}'), CAST(id AS INT)) AS Data")

# Extrai os componentes de data
df_tempo = df_calendario.selectExpr(
    "Data",
    "year(Data) AS Ano",
    "month(Data) AS Mes",
       "(CASE month(Data) \
        WHEN 1 THEN 'JANEIRO' \
        WHEN 2 THEN 'FEVEREIRO' \
        WHEN 3 THEN 'MARCO' \
        WHEN 4 THEN 'ABRIL' \
        WHEN 5 THEN 'MAIO' \
        WHEN 6 THEN 'JUNHO' \
        WHEN 7 THEN 'JULHO' \
        WHEN 8 THEN 'AGOSTO' \
        WHEN 9 THEN 'SETEMBRO' \
        WHEN 10 THEN 'OUTUBRO' \
        WHEN 11 THEN 'NOVEMBRO' \
        WHEN 12 THEN 'DEZEMBRO' \
    END) AS NomeMes",
    "day(Data) AS Dia",
    "(CASE dayofweek(Data) \
        WHEN 1 THEN 'DOMINGO' \
        WHEN 2 THEN 'SEGUNDA-FEIRA' \
        WHEN 3 THEN 'TERCA-FEIRA' \
        WHEN 4 THEN 'QUARTA-FEIRA' \
        WHEN 5 THEN 'QUINTA-FEIRA' \
        WHEN 6 THEN 'SEXTA-FEIRA' \
        WHEN 7 THEN 'SABADO' \
    END) AS NomeDiaSemana",
    "dayofweek(Data) AS NumeroDiaSemana"
)

# Exibe o DataFrame resultante
df_tempo.display()

df_tempo.write.option("path", f"/mnt/{storageAccountName}/gold/dim_tempo").saveAsTable("dim_tempo", format="delta")


# COMMAND ----------

# MAGIC %sql
# MAGIC DESCRIBE TABLE EXTENDED dim_item

# COMMAND ----------

df_item.createOrReplaceTempView("item")
df_category.createOrReplaceTempView("categoria")

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC WITH itens_relacional AS (
# MAGIC 	SELECT codigo_item,
# MAGIC 		   i.descricao,
# MAGIC 		   c.descricao as descricao_categoria,
# MAGIC 		   i.preco,
# MAGIC 		   i.ativo
# MAGIC 	  FROM item i
# MAGIC 		JOIN categoria c
# MAGIC 		     ON c.codigo_categoria = i.codigo_categoria
# MAGIC )
# MAGIC MERGE INTO
# MAGIC 	dim_item AS i
# MAGIC USING
# MAGIC 	itens_relacionas AS ir
# MAGIC ON ir.codigo_item = i.codigo_item
# MAGIC
# MAGIC WHEN MATCHED AND (i.descricao <> ir.descricao OR i.categoria <> ir.descricao_categoria OR i.preco <> ir.preco OR i.ativo <> ir.ativo) THEN
# MAGIC
# MAGIC 	UPDATE SET codigo_item = ir.codigo_item,
# MAGIC 						 descricao = ir.descricao,
# MAGIC 	           categoria = ir.descricao_categoria,
# MAGIC 			   		 preco = ir.preco,
# MAGIC 			   		 ativo = ir.ativo
# MAGIC
# MAGIC WHEN NOT MATCHED THEN
# MAGIC 	INSERT (codigo_item, descricao, categoria, preco, ativo)
# MAGIC 	VALUES (ir.codigo_item, ir.descricao, ir.descricao_categoria, ir.preco, ir.ativo)
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from dim_carro

# COMMAND ----------

# MAGIC %sql
# MAGIC drop table if exists dim_cliente

# COMMAND ----------

# MAGIC %sql
# MAGIC create table dim_cliente (
# MAGIC    SK_CLIENTE           bigint generated by default as identity,
# MAGIC    CODIGO_CLIENTE       integer,
# MAGIC    NOME                 varchar(100),
# MAGIC    TELEFONE             varchar(15),
# MAGIC    EMAIL                varchar(100),
# MAGIC    CPF                  varchar(11)
# MAGIC )
# MAGIC USING delta
# MAGIC LOCATION '/mnt/datalakedeed8f8333bc4750/gold/dim_cliente'
# MAGIC

# COMMAND ----------

df_cliente.createOrReplaceTempView("cliente")

# COMMAND ----------

# MAGIC %sql
# MAGIC --v3a (COM CTE) prevendo atualizacao SCD1
# MAGIC WITH cliente_relacional AS (
# MAGIC 	SELECT codigo_cliente,
# MAGIC 		   nome,
# MAGIC 		   telefone,
# MAGIC 		   email,
# MAGIC 		   cpf
# MAGIC 	  FROM cliente
# MAGIC )
# MAGIC MERGE INTO
# MAGIC 	dim_cliente AS d
# MAGIC USING
# MAGIC 	cliente_relacional AS r
# MAGIC ON r.codigo_cliente = d.codigo_cliente   
# MAGIC
# MAGIC WHEN MATCHED AND (r.nome <> d.nomeOR r.telefone <> d.telefone OR r.email <> d.email OR r.cpf <> d.cpf ) THEN
# MAGIC
# MAGIC 	UPDATE SET codigo_cliente = r.codigo_cliente,
# MAGIC 	           nome = r.nome,
# MAGIC 			       telefone = r.telefone,
# MAGIC 			       email = r.email,
# MAGIC 			       cpf = r.cpf
# MAGIC
# MAGIC WHEN NOT MATCHED THEN
# MAGIC
# MAGIC 	INSERT (codigo_cliente, nome, telefone, email, cpf)
# MAGIC 	VALUES (r.codigo_cliente, r.nome, r.telefone, r.email, r.cpf)
# MAGIC
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from dim_cliente

# COMMAND ----------

# MAGIC %sql
# MAGIC drop table if exists fato_

# COMMAND ----------

# MAGIC %sql
# MAGIC create table fato_ (
# MAGIC    FK_ITEM              int,
# MAGIC    FK_CLIENTE           int
# MAGIC )
# MAGIC USING delta
# MAGIC LOCATION '/mnt/datalakedeed8f8333bc4750/gold/fato_'

# COMMAND ----------

df_apolice.createOrReplaceTempView("")

# COMMAND ----------

# MAGIC %sql
# MAGIC insert into fato_
# MAGIC select 
# MAGIC from 
# MAGIC group by

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from fato_
